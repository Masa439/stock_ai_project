import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import glob
import os

# `tickers.txt` ã‹ã‚‰ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã®ã¿ã‚’å–å¾—ï¼ˆéŠ˜æŸ„åã¯ç„¡è¦–ï¼‰
tickers_file = "C:/Users/Nedet/Desktop/project_root/env/tickers.txt"
with open(tickers_file, "r", encoding="utf-8") as f:
    tickers = [line.split(",")[0].strip() for line in f.readlines() if line.strip()]

# ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ï¼ˆã™ã¹ã¦ã® `_processed.csv` ã‚’çµåˆï¼‰
data_dir = "C:/Users/Nedet/Desktop/project_root/env/data"
file_paths = [os.path.join(data_dir, f"{ticker}_stock_data_processed.csv") for ticker in tickers if os.path.exists(os.path.join(data_dir, f"{ticker}_stock_data_processed.csv"))]

df_list = []
for file_path in file_paths:
    df_temp = pd.read_csv(file_path)
    df_temp["Ticker"] = os.path.basename(file_path).split("_")[0]  # ãƒ†ã‚£ãƒƒã‚«ãƒ¼åã‚’è¿½åŠ 
    df_list.append(df_temp)

df = pd.concat(df_list, ignore_index=True)

# 'Date' ã‚’æ—¥ä»˜å‹ã«å¤‰æ›ã—ã€ä¸¦ã¹æ›¿ãˆ
df['Date'] = pd.to_datetime(df['Date'])
df = df.sort_values(by=['Date', 'Ticker'])

# **ğŸ“Œ å„éŠ˜æŸ„ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡ä¸€åŒ–**
median_size = int(df['Ticker'].value_counts().median())
df_balanced = df.groupby('Ticker', group_keys=False).apply(lambda x: x.sample(n=min(len(x), median_size), replace=False)).reset_index(drop=True)

# ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’ä½œæˆ
df_balanced['Target'] = (df_balanced.groupby("Ticker")['Close'].shift(-1) > df_balanced['Close']).astype(int)

# **ğŸ“Œ `Ticker` ã®å½±éŸ¿ã‚’ä¸‹ã’ã‚‹ãŸã‚ã«æ•°å€¤ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**
le = LabelEncoder()
df_balanced['Ticker_Label'] = le.fit_transform(df_balanced['Ticker'])

# **ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡**
features = ['Close', 'SMA_50', 'SMA_200', 'RSI', 'EMA_50', 'Ticker_Label']
X = df_balanced[features]
y = df_balanced['Target']

# **ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚’çµ±ä¸€**
scaler = StandardScaler()
X.loc[:, features] = scaler.fit_transform(X[features])

# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²ï¼ˆã‚¯ãƒ©ã‚¹ã®ãƒãƒ©ãƒ³ã‚¹ã‚’ç¶­æŒï¼‰
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# ã‚¯ãƒ©ã‚¹ã®ä¸å‡è¡¡ã‚’è£œæ­£
num_neg = sum(y_train == 0)
num_pos = sum(y_train == 1)
scale_pos_weight = max(num_neg / num_pos, 1)

# XGBoost ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
model = xgb.XGBClassifier(
    n_estimators=300,
    learning_rate=0.05,
    max_depth=5,
    subsample=0.8,
    colsample_bytree=0.8,
    scale_pos_weight=scale_pos_weight,
    use_label_encoder=False,
    eval_metric='logloss'
)

# ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
model.fit(X_train, y_train)

# äºˆæ¸¬ã¨è©•ä¾¡
y_pred = model.predict(X_test)
print(f"âœ… Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print("ğŸ“Š Classification Report:")
print(classification_report(y_test, y_pred, zero_division=1))

# ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
feature_importances = model.feature_importances_
plt.barh(features, feature_importances)
plt.xlabel('Importance')
plt.title('Feature Importance')
plt.show()
